# Standards Comparison (Group 12)

This repository contains the **implementation** for our ITMS-578 research project.  
I operationalize a set of **benchmarks (metrics)** derived from widely used AI governance sources (NIST AI RMF, OECD AI Principles, ISO/IEC 42001 themes) and **compare** three policy frameworks:

- EU AI Act (2024)
- U.S. Executive Order 14110 (2023)
- Saudi SDAIA AI Guidelines (2023â€“2025)

The code transforms qualitative policy analysis into **structured, reproducible metrics**, computes **statistics**, and outputs **visualizations** (bar charts, radar-style spokes, and a correlation heatmap).

---

## Structure

```
standards-comparison/
â”œâ”€ data/
â”‚  â”œâ”€ standards.csv                # Our dataset of frameworks Ã— metrics (editable)
â”‚  â””â”€ scoring_rubric.md            # How we mapped text â†’ numeric scores (1â€“5)
â”œâ”€ src/
â”‚  â”œâ”€ analyze.py                   # CLI: loads data, computes stats, saves plots
â”‚  â””â”€ radar.py                     # Helper for making radar/spider plots (matplotlib)
â”œâ”€ figures/                        # Output charts saved here
â”œâ”€ requirements.txt                # Python deps (minimal)
â””â”€ Standards.md                       # You are here
```

> **Note**: Scores (1â€“5) are **just an initial draft** to demonstrate the pipeline. Replace with your finalized ratings after your group finishes the document analysis.

---

## Metrics (Benchmarks)

I compare each framework on the following **metrics (1â€“5)**. These are derived from common governance themes across NIST/OECD/ISO. Adjust as needed:

1. **Human_Oversight** â€“ explicit human-in-the-loop, override, and oversight mechanisms.
2. **Transparency** â€“ documentation, disclosure, model cards, risk reporting.
3. **Accountability** â€“ clear assignment of responsibilities, auditability, liability hooks.
4. **Privacy_Protection** â€“ data governance, privacy-by-design, minimization, DPIA-like steps.
5. **Security_Robustness** â€“ security controls, red-teaming, incident response.
6. **Risk_Management** â€“ risk classification, impact assessment, mitigation cycles.
7. **Enforcement_Strength** â€“ regulatory powers, penalties, conformity assessments.
8. **Fairness_And_Bias** â€“ non-discrimination measures, bias testing and mitigation.
9. **Data_Quality** â€“ dataset governance, provenance, quality thresholds.
10. **Monitoring_Audit** â€“ monitoring in deployment, logging, auditing.

Scoring uses **1 (weak) â†’ 5 (strong)**. See `data/scoring_rubric.md` for the mapping template.

---



### Run the analysis

```bash
python src/analyze.py --input data/standards.csv --outdir figures
```

Outputs (saved in `figures/`):
- `bar_overall_scores.png`
- `heatmap_metric_correlations.png`
- `radar_EU_AI_Act_2024.png`, `radar_US_EO_14110_2023.png`, `radar_Saudi_SDAIA_2023_2025.png`

---

## How We Scored (Workflow Draft)

1. Read policy text and **extract evidence** per metric (quotes/sections).
2. Map evidence to a **score 1â€“5** using the rubric.
3. Double-review scores (two group members) to reduce subjectivity.
4. Record traceability: **framework â†’ metric â†’ evidence â†’ score**.
5. Re-run `analyze.py` to regenerate figures for the paper.

---

# ğŸ“Š Scoring Justification Table â€” AI Regulation Benchmark

This table provides transparent justification for the **metric scores** assigned to each AI regulatory framework â€” namely:

- **EU Artificial Intelligence Act (2024)**
- **U.S. Executive Order 14110 (2023)**
- **Saudi SDAIA AI Framework (2023â€“2025)**

The scores (1â€“5) were derived through a structured **text-based analysis** of each document. Each metric maps to a legal article or policy clause, with qualitative interpretation of its **strength, enforcement clarity, and ethical alignment**.

---

| **Metric** | **EU AI Act (2024)** | **US EO 14110 (2023)** | **Saudi SDAIA (2023â€“25)** | **Legal / Text Evidence** | **Explanation (Why this score?)** |
|-------------|---------------------|--------------------------|-----------------------------|----------------------------|-----------------------------------|
| Human Oversight | 5 | 4 | 3 | EU Art. 14; EO Â§ 4(d); SDAIA Principle #3 | EU mandates human-in-loop for high-risk AI. US encourages oversight but voluntary; Saudi mentions it without strict enforcement. |
| Transparency | 4 | 3 | 3 | EU Art. 52; EO Â§ 8(a); SDAIA Principle #2 | EU requires disclosure and explainability. US promotes transparency but leaves implementation flexible. Saudi encourages but lacks legal force. |
| Accountability | 5 | 4 | 3 | EU Art. 71; EO Â§ 10(b); SDAIA Framework Â§3 | EU assigns liability for misuse. US delegates to agencies. Saudi assigns moral accountability. |
| Privacy Protection | 5 | 4 | 4 | GDPR; EO Â§ 9; SDAIA AI Â§ 2.3 | EU links AI rules directly to GDPR. US references privacy laws; Saudi enforces national data laws. |
| Security & Robustness | 4 | 5 | 4 | EU Annex IV; EO Â§ 4(b); SDAIA AI Â§ 3.1 | EU enforces testing tiers; US promotes cybersecurity best practices; Saudi applies resilience standards. |
| Risk Management | 5 | 4 | 4 | EU Art. 6; EO Â§ 5; SDAIA AI Â§ 4.1 | EU uses proportional risk tiers. US introduces NIST risk controls. Saudi defines AI risk taxonomy. |
| Enforcement Strength | 5 | 3 | 3 | EU Art. 99; EO Â§ 10; SDAIA AI Â§ 5.2 | EU allows penalties up to 6% turnover. US limited to administrative actions. Saudi compliance is advisory. |
| Fairness & Bias | 4 | 4 | 3 | EU Annex IV (2); EO Â§ 7(c); SDAIA AI Â§ 3.2 | EU mandates bias testing. US encourages fairness principles. Saudi references ethics but no binding test. |
| Data Quality | 4 | 4 | 4 | EU Annex IV (3); NIST RMF Â§ 2.3; SDAIA AI Â§ 3.3 | All ensure dataset quality; EU has audit protocols. |
| Monitoring & Audit | 4 | 4 | 4 | EU Art. 61; EO Â§ 8(b); SDAIA AI Â§ 5.4 | EU requires post-market monitoring. US builds AI Safety Institute. Saudi establishes auditing committee. |

---

### ğŸ“˜ References
- **EU Artificial Intelligence Act (2024)** â€” Regulation (EU) 2024/1689. [EUR-Lex](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)  
- **U.S. Executive Order 14110 (2023)** â€” The White House. [Link](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/)  
- **Saudi SDAIA AI Ethics Framework (2023â€“2025)** â€” [SDAIA AI Ethics](https://ai.sa/AI-Ethics)  
- **OECD AI Principles (2019)** â€” [OECD Principles](https://oecd.ai/en/ai-principles)  
- **NIST AI Risk Management Framework (2023)** â€” [NIST RMF](https://www.nist.gov/itl/ai-risk-management-framework)

---

### ğŸ” How to Interpret Scores and Figures
Each metric is **scored from 1â€“5** based on **evidence strength and enforcement depth**:
- 5ï¸âƒ£ = Legally binding and detailed enforcement mechanisms  
- 4ï¸âƒ£ = Strong policy guidance or delegated enforcement  
- 3ï¸âƒ£ = Ethical or advisory principles only  
- 2ï¸âƒ£ = Minimal reference, weak enforcement  
- 1ï¸âƒ£ = No coverage or unclear policy

These values were visualized using **radar charts**, **bar graphs**, and **heatmaps** located in:
```
/khalid/standards-comparison/figures/
```
Each figure corresponds to how each framework performs across all 10 ethical and regulatory metrics.